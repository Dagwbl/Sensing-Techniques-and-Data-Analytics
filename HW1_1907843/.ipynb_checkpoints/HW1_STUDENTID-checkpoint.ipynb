{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c6b883f",
   "metadata": {},
   "source": [
    "# HW1 — Smartphone-Based Sensing of Pendulum Vibrations\n",
    "Instructor: Mohammad Talebi-Kalaleh  \n",
    "Course: Sensing Techniques and Data Analytics — Fall 2025\n",
    "\n",
    "**Student ID:** `STUDENT_ID`  \n",
    "**Name:** Your Name Here  \n",
    "**Date:** YYYY-MM-DD  \n",
    "\n",
    "----\n",
    "\n",
    "This notebook is a ready-to-use framework for Assignment 1. Replace placeholders, run cells, and fill in your analysis & results. Follow the submission instructions at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab402d3f",
   "metadata": {},
   "source": [
    "## Notebook Outline\n",
    "\n",
    "1. Setup & Imports  \n",
    "2. Utility functions (data loading, peak detection, damping fit, stats)  \n",
    "3. Part A — Short Pendulum (L ≈ 0.3 m)  \n",
    "4. Part B — Long Pendulum (L ≈ 1.0 m)  \n",
    "5. Part C — Nonlinear Regime (large angles)  \n",
    "6. Part D — Noise discussion  \n",
    "7. Figures & Data saving helpers  \n",
    "8. Submission checklist\n",
    "\n",
    "Each analysis part includes: data loading, visualization, statistical descriptors, period estimation, damping estimation, and discussion cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc9df85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and plotting settings\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.signal as signal\n",
    "import scipy.optimize as optimize\n",
    "from scipy.stats import skew, kurtosis\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure figures render in notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Reproducible plotting size policy\n",
    "plt.rcParams['figure.figsize'] = (10,4)\n",
    "\n",
    "# Paths (edit STUDENT_ID below)\n",
    "STUDENT_ID = \"STUDENT_ID\"\n",
    "BASE_DIR = f\"HW1_{STUDENT_ID}\"\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'Data')\n",
    "FIG_DIR = os.path.join(BASE_DIR, 'Figures')\n",
    "\n",
    "# Create directories if not present (useful when running locally)\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(FIG_DIR, exist_ok=True)\n",
    "\n",
    "print('Data directory:', DATA_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808f988c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "\n",
    "def load_trial_csv(path):\n",
    "    \"\"\"Load CSV exported from PhyPhox. \n",
    "    Assumes time column 'time' (s) and acceleration axis 'accel' aligned with swing direction.\n",
    "    Adjust column names below if your CSV uses different headers.\"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    # Try common column names\n",
    "    for col in ['time', 't', 'Time']:\n",
    "        if col in df.columns:\n",
    "            time_col = col\n",
    "            break\n",
    "    else:\n",
    "        time_col = df.columns[0]\n",
    "    # choose accel axis heuristically\n",
    "    accel_col = None\n",
    "    for col in ['accel_x', 'accel_y', 'accel_z', 'acceleration', 'acc']:\n",
    "        if col in df.columns:\n",
    "            accel_col = col\n",
    "            break\n",
    "    if accel_col is None:\n",
    "        accel_col = df.columns[1] if df.shape[1] > 1 else df.columns[0]\n",
    "    df = df.rename(columns={time_col: 'time', accel_col: 'accel'})\n",
    "    return df[['time','accel']]\n",
    "\n",
    "def compute_descriptors(x):\n",
    "    return {\n",
    "        'mean': np.mean(x),\n",
    "        'variance': np.var(x, ddof=1),\n",
    "        'rms': np.sqrt(np.mean(np.square(x))),\n",
    "        'skewness': skew(x),\n",
    "        'kurtosis': kurtosis(x, fisher=False)  # Pearson's definition\n",
    "    }\n",
    "\n",
    "def estimate_period_via_peaks(time, signal_data, prominence=0.5, distance=None):\n",
    "    # find peaks (positive swings)\n",
    "    peaks, props = signal.find_peaks(signal_data, prominence=prominence, distance=distance)\n",
    "    if len(peaks) < 2:\n",
    "        return np.nan, peaks\n",
    "    peak_times = time[peaks]\n",
    "    periods = np.diff(peak_times)\n",
    "    return periods, peaks\n",
    "\n",
    "def fit_exponential_decay(peak_times, peak_amps, Tn=None):\n",
    "    \"\"\"Fit A(t) = A0 * exp(-zeta * 2*pi / Tn * t)\n",
    "    If Tn is None, fit exponent parameter directly as A0 * exp(-k t) and compute zeta from k and Tn supplied per-trial.\n",
    "    Returns dict with A0, zeta (if possible), k (decay rate), covariance.\n",
    "    \"\"\"\n",
    "    # fit ln(A) = ln(A0) - k t  where k = zeta * 2*pi / Tn\n",
    "    positive_mask = peak_amps > 0\n",
    "    t = np.array(peak_times)[positive_mask]\n",
    "    A = np.array(peak_amps)[positive_mask]\n",
    "    if len(A) < 2:\n",
    "        return {'A0': np.nan, 'k': np.nan, 'zeta': np.nan}\n",
    "    logA = np.log(A)\n",
    "    coef = np.polyfit(t, logA, 1)\n",
    "    k = -coef[0]  # because logA = ln(A0) - k t\n",
    "    A0 = np.exp(coef[1])\n",
    "    zeta = np.nan\n",
    "    if Tn is not None and Tn>0:\n",
    "        zeta = k * Tn / (2*np.pi)\n",
    "    return {'A0': A0, 'k': k, 'zeta': zeta}\n",
    "\n",
    "def theoretical_period(L, g=9.81):\n",
    "    return 2*np.pi*np.sqrt(L/g)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47075d10",
   "metadata": {},
   "source": [
    "## Part A — Short Pendulum (L ≈ 0.3 m)\n",
    "\n",
    "**Instructions:** Place CSV files for Part A into `HW1_STUDENTID/Data/` with names like `partA_trial01.csv` ... `partA_trial10.csv`. The code cells below will loop through trials, compute descriptors, estimate period, fit damping, and summarize results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfc1841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part A analysis loop (short pendulum)\n",
    "L_short = 0.3  # meters (adjust if measured)\n",
    "file_pattern = os.path.join(DATA_DIR, 'partA_trial*.csv')\n",
    "files = sorted(glob.glob(file_pattern))\n",
    "results = []\n",
    "\n",
    "for fpath in files:\n",
    "    df = load_trial_csv(fpath)\n",
    "    t = df['time'].values\n",
    "    x = df['accel'].values\n",
    "    # basic plot\n",
    "    plt.figure(); plt.plot(t, x); plt.title(f'Raw accel: {os.path.basename(fpath)}'); plt.xlabel('time (s)'); plt.ylabel('accel (units)'); plt.tight_layout()\n",
    "    # descriptors\n",
    "    desc = compute_descriptors(x)\n",
    "    # smoothing for peak detection (optional)\n",
    "    x_smooth = signal.savgol_filter(x, 11, 3)\n",
    "    periods, peaks = estimate_period_via_peaks(t, x_smooth, prominence=np.std(x_smooth)*0.5)\n",
    "    if isinstance(periods, np.ndarray):\n",
    "        mean_period = np.mean(periods)\n",
    "        std_period = np.std(periods, ddof=1)\n",
    "    else:\n",
    "        mean_period = np.nan\n",
    "        std_period = np.nan\n",
    "    # peak amplitudes and times\n",
    "    peak_times = t[peaks] if len(peaks)>0 else np.array([])\n",
    "    peak_amps = np.abs(x_smooth[peaks]) if len(peaks)>0 else np.array([])\n",
    "    decay = fit_exponential_decay(peak_times, peak_amps, Tn=mean_period)\n",
    "    result = {\n",
    "        'file': os.path.basename(fpath),\n",
    "        'mean_period': mean_period,\n",
    "        'std_period': std_period,\n",
    "        'A0': decay['A0'],\n",
    "        'k': decay['k'],\n",
    "        'zeta': decay['zeta'],\n",
    "        **desc\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bcef78",
   "metadata": {},
   "source": [
    "## Part B — Long Pendulum (L ≈ 1.0 m)\n",
    "\n",
    "Repeat analysis from Part A for `partB_trialXX.csv` files. Adjust `L_long` below if your measured length differs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a801727",
   "metadata": {},
   "outputs": [],
   "source": [
    "L_long = 1.0\n",
    "file_pattern = os.path.join(DATA_DIR, 'partB_trial*.csv')\n",
    "files = sorted(glob.glob(file_pattern))\n",
    "# Copy-paste the same loop for Part B (or refactor into a function). For brevity we reuse code:\n",
    "results_B = []\n",
    "for fpath in files:\n",
    "    df = load_trial_csv(fpath)\n",
    "    t = df['time'].values\n",
    "    x = df['accel'].values\n",
    "    x_smooth = signal.savgol_filter(x, 11, 3)\n",
    "    periods, peaks = estimate_period_via_peaks(t, x_smooth, prominence=np.std(x_smooth)*0.5)\n",
    "    if isinstance(periods, np.ndarray):\n",
    "        mean_period = np.mean(periods)\n",
    "        std_period = np.std(periods, ddof=1)\n",
    "    else:\n",
    "        mean_period = np.nan\n",
    "        std_period = np.nan\n",
    "    peak_times = t[peaks] if len(peaks)>0 else np.array([])\n",
    "    peak_amps = np.abs(x_smooth[peaks]) if len(peaks)>0 else np.array([])\n",
    "    decay = fit_exponential_decay(peak_times, peak_amps, Tn=mean_period)\n",
    "    desc = compute_descriptors(x)\n",
    "    result = {\n",
    "        'file': os.path.basename(fpath),\n",
    "        'mean_period': mean_period,\n",
    "        'std_period': std_period,\n",
    "        'A0': decay['A0'],\n",
    "        'k': decay['k'],\n",
    "        'zeta': decay['zeta'],\n",
    "        **desc\n",
    "    }\n",
    "    results_B.append(result)\n",
    "pd.DataFrame(results_B)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72523a88",
   "metadata": {},
   "source": [
    "## Part C — Nonlinear Regime (Large Angles)\n",
    "\n",
    "Use `partC_trialXX.csv`. For large angle trials (θ0 > 45°), measure period variation vs amplitude and compare with small-angle theory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3450376",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_pattern = os.path.join(DATA_DIR, 'partC_trial*.csv')\n",
    "files = sorted(glob.glob(file_pattern))\n",
    "period_vs_amp = []\n",
    "for fpath in files:\n",
    "    df = load_trial_csv(fpath)\n",
    "    t = df['time'].values\n",
    "    x = df['accel'].values\n",
    "    x_smooth = signal.savgol_filter(x, 11, 3)\n",
    "    periods, peaks = estimate_period_via_peaks(t, x_smooth, prominence=np.std(x_smooth)*0.5)\n",
    "    peak_times = t[peaks] if len(peaks)>0 else np.array([])\n",
    "    peak_amps = np.abs(x_smooth[peaks]) if len(peaks)>0 else np.array([])\n",
    "    if len(peak_amps)>0 and isinstance(periods, np.ndarray):\n",
    "        period_vs_amp.append({'file': os.path.basename(fpath), 'mean_period': np.mean(periods), 'mean_amp': np.mean(peak_amps)})\n",
    "pd.DataFrame(period_vs_amp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9ca3a4",
   "metadata": {},
   "source": [
    "## Part D — Noise Discussion\n",
    "\n",
    "Visually inspect signals and write your qualitative assessment here. Use the examples shown in previous plots. Answer: What are possible noise sources? How do they affect period/damping estimation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6878c983",
   "metadata": {},
   "source": [
    "## Submission Checklist\n",
    "\n",
    "- [ ] Export this notebook to PDF: `HW1_{STUDENT_ID}.pdf` (max 20 A4 pages)\n",
    "- [ ] Create ZIP named `HW1_{STUDENT_ID}.zip` containing:\n",
    "  - Executed `.ipynb`\n",
    "  - `Data/` folder with CSVs\n",
    "  - `Figures/` folder with generated plots\n",
    "- [ ] Verify all tables are rendered as DataFrames (no screenshots)\n",
    "- [ ] Confirm all equations and figures are numbered and captioned\n",
    "- [ ] Ensure reproducibility: all code cells run from top to bottom without manual path edits (except `STUDENT_ID`)\n",
    "\n",
    "**Good luck!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bc5d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: save summary tables and figures\n",
    "summary_csv = os.path.join(BASE_DIR, f'summary_{STUDENT_ID}.csv')\n",
    "if 'results_df' in globals():\n",
    "    results_df.to_csv(os.path.join(BASE_DIR, 'summary_partA.csv'), index=False)\n",
    "if 'results_B' in globals():\n",
    "    pd.DataFrame(results_B).to_csv(os.path.join(BASE_DIR, 'summary_partB.csv'), index=False)\n",
    "print('Saved summary CSVs to', BASE_DIR)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

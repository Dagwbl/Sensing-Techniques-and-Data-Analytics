{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "Author: Jeapo\n",
        "title: Smartphone - Based Sensing of Pendulum Vibrations\n",
        "jupyter: python3\n",
        "format: \n",
        "    html: \n",
        "        toc: true\n",
        "        number-sections: true\n",
        "        html-math-method: katex\n",
        "        toc-expand: 3\n",
        "    pdf: \n",
        "        toc: true\n",
        "        number-sections: true\n",
        "        colorlinks: true\n",
        "\n",
        "crossref:\n",
        "  chapters: true\n",
        "editor: \n",
        "    render-on-save: true\n",
        "---\n",
        "\n",
        "**Instructor:** Mohammad Talebi-Kalaleh  \n",
        "**Course:** Sensing Techniques and Data Analytics — Fall 2025\n",
        "\n",
        "**Student ID:** `STUDENT_ID`  \n",
        "**Name:** Your Name Here  \n",
        "**Date:** YYYY-MM-DD  \n",
        "\n",
        "**Tool Chains**: `Jupyter`->`Quarto`->`PDF`\n",
        "\n",
        "\n",
        "\n",
        "# Background\n",
        "## Assignment Outline\n",
        "\n",
        "1. Setup & Imports  \n",
        "2. Utility functions (data loading, peak detection, damping fit, stats)  \n",
        "3. Part A — Short Pendulum (L ≈ 0.3 m)  \n",
        "4. Part B — Long Pendulum (L ≈ 1.0 m)  \n",
        "5. Part C — Nonlinear Regime (large angles)  \n",
        "6. Part D — Noise discussion  \n",
        "7. Figures & Data saving helpers  \n",
        "8. Submission checklist\n",
        "\n",
        "Each analysis part includes: data loading, visualization, statistical descriptors, period estimation, damping estimation, and discussion cells."
      ],
      "id": "ffc563a6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: Imports and settings\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.signal as signal\n",
        "import scipy.optimize as optimize\n",
        "from scipy.stats import skew, kurtosis\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# Ensure figures render in notebook\n",
        "%matplotlib inline\n",
        "\n",
        "# Reproducible plotting size policy\n",
        "plt.rcParams['figure.figsize'] = (10,4)\n",
        "\n",
        "# Paths (edit STUDENT_ID below)\n",
        "STUDENT_ID = \"STUDENT_ID\"\n",
        "BASE_DIR = f\".\"\n",
        "DATA_DIR = os.path.join(BASE_DIR, 'Data')\n",
        "FIG_DIR = os.path.join(BASE_DIR, 'Figures')"
      ],
      "id": "Imports-and-settings",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment Preparation\n",
        "\n",
        "### Materials\n",
        "1. Smartphone:\n",
        "    Type: Xiaomi 10\n",
        "    Target sensor: accelerator\n",
        "2. String\n",
        "3. Toilet paper roll\n",
        "4. Camera: Another smartphone\n",
        "5. Softer & Version: PhyPhox 1.2.0\n",
        "6. Cross beam: 1 Chopstick\n",
        "6. Angle measurement: Compass App of Smartphone\n",
        "\n",
        "### Structure\n",
        "\n",
        "The pendulum device was constructed as follows:\n",
        "\n",
        "- A chopstick was used as the cross beam and fixed horizontally to serve as the support.\n",
        "- Four lengths of string were cut and tied to the chopstick, spaced evenly to ensure stability.\n",
        "- The lower ends of the strings were attached to a toilet paper roll, which acted as the pendulum bob holder.\n",
        "- A smartphone (Xiaomi 10) was placed securely inside the toilet paper roll, with its main axis (y) aligned along the swing direction.\n",
        "- The PhyPhox app (version 1.2.0) was installed on the smartphone to record acceleration data.\n",
        "- A second smartphone was used as a camera to record the motion for reference and angle measurement.\n",
        "- The initial angle of displacement was measured using the compass app on the smartphone before release.\n",
        "\n",
        "This setup allowed for accurate measurement of pendulum motion using the smartphone's accelerometer, while the camera provided visual confirmation and angle calibration.\n",
        "\n",
        "## Theoretical Model\n",
        "\n",
        "### Small-Angle Pendulum Period\n",
        "\n",
        "For small angular displacements ($\\theta \\le 15^\\circ$), the period of a simple pendulum is given by @eq-simple-pendulum:\n",
        "$$\n",
        "T_{n}=2\\pi\\sqrt{\\frac{L}{g}}\n",
        "$${#eq-simple-pendulum}\n",
        "\n",
        "where:\n",
        "\n",
        "  * $T\\_n$ = oscillation period (seconds)\n",
        "  * $L$ = pendulum length (meters)\n",
        "  * $g$ = gravitational acceleration ($9.81, m/s^2$)"
      ],
      "id": "0afc4a90"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: Theoretical data for comparison\n",
        "def generate_theoretical_pendulum(L, t_max=10, dt=0.01, theta0=15, g=9.81):\n",
        "    \"\"\"Generate theoretical pendulum motion for small angles\n",
        "    Args:\n",
        "        L (float): pendulum length in meters\n",
        "        t_max (float): simulation duration in seconds\n",
        "        dt (float): time step in seconds\n",
        "        theta0 (float): initial angle in radians\n",
        "        g (float): gravitational acceleration (m/s^2)\n",
        "    Returns:\n",
        "        t, theta: time and angle arrays\n",
        "    \"\"\"\n",
        "    t = np.arange(0, t_max, dt)\n",
        "    omega = np.sqrt(g/L)  # natural frequency\n",
        "    theta = theta0 * np.cos(omega * t)  # solution for small angles\n",
        "    return t, theta\n",
        "\n",
        "def theoretical_period(L, g=9.81):\n",
        "    \"\"\"Calculate theoretical period of a simple pendulum\"\"\"\n",
        "    return 2 * np.pi * np.sqrt(L / g)\n",
        "\n",
        "# Generate and plot theoretical data for both pendulum lengths\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 4))\n",
        "\n",
        "# Short pendulum\n",
        "L_short = 0.3 # part A\n",
        "t_short, theta_short = generate_theoretical_pendulum(L_short)\n",
        "T_short = theoretical_period(L_short)\n",
        "ax1.plot(t_short, theta_short, 'b-', label=f'Theory (T={T_short:.3f}s)')\n",
        "ax1.set_title(f'Short Pendulum (L={L_short}m)')\n",
        "ax1.set_xlabel('Time (s)')\n",
        "ax1.set_ylabel('Angle (rad)')\n",
        "ax1.grid(True)\n",
        "ax1.legend()\n",
        "\n",
        "# Long pendulum\n",
        "L_long = 1.0 # part B\n",
        "t_long, theta_long = generate_theoretical_pendulum(L_long)\n",
        "T_long = theoretical_period(L_long)\n",
        "ax2.plot(t_long, theta_long, 'r-', label=f'Theory (T={T_long:.3f}s)')\n",
        "ax2.set_title(f'Long Pendulum (L={L_long}m)')\n",
        "ax2.set_xlabel('Time (s)')\n",
        "ax2.set_ylabel('Angle (rad)')\n",
        "ax2.grid(True)\n",
        "ax2.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "print(f'Theoretical periods:\\nShort pendulum: {T_short:.3f}s\\nLong pendulum: {T_long:.3f}s')"
      ],
      "id": "Theoretical-data-for-comparison",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Damped Oscillation Model\n",
        "\n",
        "The amplitude envelope of a damped pendulum follows exponential decay, as shown in @eq-damped-pendulum:\n",
        "$$\n",
        "A(t)=A_{0} \\exp(-\\zeta\\frac{2\\pi}{T_{n}}t)\n",
        "$$ {#eq-damped-pendulum}\n",
        "\n",
        "where:\n",
        "\n",
        "  * $A(t)$ = amplitude at time t\n",
        "  * $A_0$ = initial amplitude\n",
        "  * $\\zeta$ = damping ratio (dimensionless)\n",
        "  * $t$ = elapsed time (seconds)"
      ],
      "id": "14da0fb5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: Damped Oscillation\n",
        "def generate_damped_pendulum(L, t_max=10, dt=0.01, theta0=15, zeta=0.1, g=9.81):\n",
        "    \"\"\"Generate damped pendulum motion\n",
        "    Args:\n",
        "        L (float): pendulum length in meters\n",
        "        t_max (float): simulation duration in seconds\n",
        "        dt (float): time step in seconds\n",
        "        theta0 (float): initial angle in radians\n",
        "        zeta (float): damping ratio (dimensionless)\n",
        "        g (float): gravitational acceleration (m/s^2)\n",
        "    Returns:\n",
        "        t, theta: time and angle arrays\n",
        "    \"\"\"\n",
        "    t = np.arange(0, t_max, dt)\n",
        "    omega = np.sqrt(g/L)  # natural frequency\n",
        "    # Damped solution\n",
        "    theta = theta0 * np.exp(-zeta * omega * t) * np.cos(omega * t)\n",
        "    return t, theta\n",
        "\n",
        "# Plot comparison of ideal vs damped oscillation\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 6))\n",
        "\n",
        "# Short pendulum comparison\n",
        "t_short_damped, theta_short_damped = generate_damped_pendulum(L_short, zeta=0.1)\n",
        "ax1.plot(t_short, theta_short, 'b-', label='Ideal', alpha=0.7)\n",
        "ax1.plot(t_short_damped, theta_short_damped, 'r--', label='Damped (ζ=0.1)')\n",
        "ax1.set_title(f'Short Pendulum (L={L_short}m)')\n",
        "ax1.set_xlabel('Time (s)')\n",
        "ax1.set_ylabel('Angle (rad)')\n",
        "ax1.grid(True)\n",
        "\n",
        "# Long pendulum comparison\n",
        "t_long_damped, theta_long_damped = generate_damped_pendulum(L_long, zeta=0.1)\n",
        "ax2.plot(t_long, theta_long, 'b-', label='Ideal', alpha=0.7)\n",
        "ax2.plot(t_long_damped, theta_long_damped, 'r--', label='Damped (ζ=0.1)')\n",
        "ax2.set_title(f'Long Pendulum (L={L_long}m)')\n",
        "ax2.set_xlabel('Time (s)')\n",
        "ax2.set_ylabel('Angle (rad)')\n",
        "ax2.grid(True)\n",
        "\n",
        "# Add the +- A_0 exp(-zeta 2pi/T_n t) lines\n",
        "for ax, t, theta0, Tn in [\n",
        "    (ax1, t_short_damped, 15, theoretical_period(L_short)),\n",
        "    (ax2, t_long_damped, 15, theoretical_period(L_long))\n",
        "]:\n",
        "    envelope = theta0 * np.exp(-0.1 * 2 * np.pi / Tn * t)\n",
        "    ax.plot(t, envelope, 'k:', label=r'$+A_0 e^{-\\zeta 2\\pi/T_n t}$')\n",
        "    ax.plot(t, -envelope, 'k:', label=r'$-A_0 e^{-\\zeta 2\\pi/T_n t}$')\n",
        "\n",
        "ax1.legend(loc='upper right')\n",
        "ax2.legend(loc='upper right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "Damped-Oscillation",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Preprocessing"
      ],
      "id": "c44a9c52"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: extract and rename data\n",
        "#| echo: false\n",
        "import zipfile\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "def extract_raw_data_from_zip_files(folder_path, experiment):\n",
        "    \"\"\"\n",
        "    Finds all zip files in a folder, extracts 'Raw Data.csv' from each,\n",
        "    and saves it with the zip file's name in a new 'extracted_data' folder.\n",
        "\n",
        "    Args:\n",
        "        folder_path (str): The path to the folder containing zip files.\n",
        "    \"\"\"\n",
        "    # Define the name of the CSV file and the output directory\n",
        "    csv_filename = \"Raw Data.csv\"\n",
        "    output_dir = os.path.join(folder_path,\"\")\n",
        "\n",
        "    # Create the output directory if it doesn't exist\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "        print(f\"Created output directory: {output_dir}\")\n",
        "\n",
        "    # Find all zip files in the specified folder\n",
        "    zip_files = glob.glob(os.path.join(output_dir,experiment, \"*.zip\"))\n",
        "    \n",
        "    if not zip_files:\n",
        "        print(\"No zip files found in the specified folder. Exiting.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Found {len(zip_files)} zip files to process.\")\n",
        "\n",
        "    for zip_filepath in zip_files:\n",
        "        try:\n",
        "            with zipfile.ZipFile(zip_filepath, 'r') as zip_ref:\n",
        "                # Get the base name of the zip file without the extension\n",
        "                base_name = os.path.splitext(os.path.basename(zip_filepath))[0]\n",
        "                \n",
        "                # Find the path to the CSV file inside the zip, accounting for nesting\n",
        "                csv_member_path = None\n",
        "                for member in zip_ref.namelist():\n",
        "                    if member.endswith(csv_filename):\n",
        "                        csv_member_path = member\n",
        "                        break\n",
        "                \n",
        "                if not csv_member_path:\n",
        "                    print(f\"Warning: '{csv_filename}' not found in '{os.path.basename(zip_filepath)}'. Skipping.\")\n",
        "                    continue\n",
        "                \n",
        "                # Extract the file to a temporary location\n",
        "                temp_extract_dir = os.path.join(output_dir, base_name)\n",
        "                zip_ref.extract(csv_member_path, temp_extract_dir)\n",
        "                \n",
        "                # The extracted file's path\n",
        "                extracted_file_path = os.path.join(temp_extract_dir, csv_member_path)\n",
        "\n",
        "                # Define the new filename and path\n",
        "                new_csv_filename = f\"{base_name}.csv\"\n",
        "                new_csv_path = os.path.join(output_dir, new_csv_filename)\n",
        "                \n",
        "                # Move and rename the extracted file\n",
        "                os.rename(extracted_file_path, new_csv_path)\n",
        "\n",
        "                # Clean up the temporary directory structure\n",
        "                os.rmdir(os.path.dirname(extracted_file_path))\n",
        "\n",
        "                print(f\"Successfully extracted and renamed '{os.path.basename(zip_filepath)}' to '{new_csv_filename}'.\")\n",
        "\n",
        "        except (zipfile.BadZipFile, FileNotFoundError) as e:\n",
        "            print(f\"Error processing '{os.path.basename(zip_filepath)}': {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred with '{os.path.basename(zip_filepath)}': {e}\")\n",
        "\n",
        "if __name__ == \"__main__\" and False:\n",
        "    folder_path = \"./Data\"\n",
        "    experiment_type = \"/Raw/partC\"\n",
        "    \n",
        "    # Run the function to process the files\n",
        "    extract_raw_data_from_zip_files(folder_path, experiment=experiment_type)"
      ],
      "id": "extract-and-rename-data",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: load_trial_csv\n",
        "\n",
        "def load_trial_csv(path,start=500,end=10000):\n",
        "    \"\"\"Load CSV exported from PhyPhox. \n",
        "    Assumes time column 'time' (s) and acceleration axis 'accel' aligned with swing direction.\n",
        "    Adjust column names below if your CSV uses different headers.\"\"\"\n",
        "    df = pd.read_csv(path)\n",
        "    time_col = df.columns[0]\n",
        "    # choose accel axis heuristically\n",
        "    accel_col = df.columns[2]\n",
        "    df = df.rename(columns={time_col: 'time', accel_col: 'accel'})\n",
        "    return df[['time','accel']].iloc[start:end]  # trim start/end"
      ],
      "id": "load_trial_csv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: compute descriptors\n",
        "#| echo: false\n",
        "#| output: false\n",
        "def compute_descriptors(x):\n",
        "    return {\n",
        "        'Mean (µ)': np.mean(x),\n",
        "        'Variance (σ²)': np.var(x, ddof=1),\n",
        "        'Root Mean Square (RMS)': np.sqrt(np.mean(np.square(x))),\n",
        "        'Skewness (γ₁)': skew(x),\n",
        "        'Kurtosis (γ₂)': kurtosis(x, fisher=False)\n",
        "    }"
      ],
      "id": "compute-descriptors",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Peak-detection algorithms\n",
        "\n",
        "The estimate_period_via_peaks function is designed to estimate the periods of oscillatory signals by identifying the time intervals between successive peaks in the data. It uses the find_peaks method from the scipy.signal module to locate prominent peaks in the signal, then calculates the time differences between these peaks to determine the periods. This approach is commonly used for analyzing periodic signals, such as those from pendulum or vibration experiments.\n",
        "\n",
        "Key points:\n",
        "\n",
        "Inputs:\n",
        "time: Array of time values.\n",
        "signal_data: Array of signal measurements (e.g., acceleration).\n",
        "prominence and distance: Parameters to control peak detection sensitivity.\n",
        "Outputs:\n",
        "periods: Array of estimated periods (time differences between peaks).\n",
        "peaks: Indices of detected peaks.\n",
        "This function is useful for extracting the fundamental period of a signal in experimental data analysis."
      ],
      "id": "d7a6df4f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: estimate period via peaks\n",
        "def estimate_period_via_peaks(time, signal_data, prominence=0.5, distance=None):\n",
        "    # find peaks (positive swings)\n",
        "    peaks, props = signal.find_peaks(signal_data, prominence=prominence, distance=distance)\n",
        "    if len(peaks) < 2:\n",
        "        return np.nan, peaks\n",
        "    peak_times = time[peaks]\n",
        "    periods = np.diff(peak_times)\n",
        "    return periods, peaks"
      ],
      "id": "estimate-period-via-peaks",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fit exponential decay\n",
        "def fit_exponential_decay(peak_times, peak_amps, Tn=None):\n",
        "    \"\"\"Fit A(t) = A0 * exp(-zeta * 2*pi / Tn * t)\n",
        "    If Tn is None, fit exponent parameter directly as A0 * exp(-k t) and compute zeta from k and Tn supplied per-trial.\n",
        "    Returns dict with A0, zeta (if possible), k (decay rate), covariance.\n",
        "    \"\"\"\n",
        "    # fit ln(A) = ln(A0) - k t  where k = zeta * 2*pi / Tn\n",
        "    positive_mask = peak_amps > 0\n",
        "    t = np.array(peak_times)[positive_mask]\n",
        "    A = np.array(peak_amps)[positive_mask]\n",
        "    if len(A) < 2:\n",
        "        return {'A0': np.nan, 'k': np.nan, 'zeta': np.nan}\n",
        "    logA = np.log(A)\n",
        "    coef = np.polyfit(t, logA, 1)\n",
        "    k = -coef[0]  # because logA = ln(A0) - k t\n",
        "    A0 = np.exp(coef[1])\n",
        "    zeta = np.nan\n",
        "    if Tn is not None and Tn>0:\n",
        "        zeta = k * Tn / (2*np.pi)\n",
        "    return {'A0': A0, 'k': k, 'zeta': zeta}"
      ],
      "id": "fit-exponential-decay",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: theoretical period\n",
        "def theoretical_period(L, g=9.81):\n",
        "    return 2*np.pi*np.sqrt(L/g)"
      ],
      "id": "theoretical-period",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part A: Short Pendulum (L ≈ 0.3 m)\n",
        "\n",
        "**Instructions:** Place CSV files for Part A into `HW1_STUDENTID/Data/` with names like `partA_trial01.csv` ... `partA_trial10.csv`. The code cells below will loop through trials, compute descriptors, estimate period, fit damping, and summarize results.\n",
        "\n",
        "## Data Collection"
      ],
      "id": "8c60426d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "file_pattern = os.path.join(DATA_DIR, 'partA_trial*.csv')\n",
        "files = sorted(glob.glob(file_pattern))"
      ],
      "id": "931ec9f9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Statistical Analysis\n",
        "\n",
        "For part A, I collect 11 trails, For each trial, compute:\n",
        "• Mean (µ)\n",
        "• Variance (σ2)\n",
        "• Root Mean Square (RMS)\n",
        "• Skewness (γ1)\n",
        "• Kurtosis (γ2)\n",
        "\n",
        "List all Part A trials and compute descriptors, output as @tbl-part-a, for assuring the data consistence, exclude the begining and the ending data, only keep the middle (from t=500 to t=10000)"
      ],
      "id": "b88c0fb0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tbl-part-a\n",
        "#| tbl-cap: Summary of Part A\n",
        "#| output: true\n",
        "#| echo: false\n",
        "summary = []\n",
        "\n",
        "for fpath in files:\n",
        "    df = load_trial_csv(fpath)\n",
        "    x = df['accel'].values\n",
        "    desc = compute_descriptors(x)\n",
        "    summary.append({\n",
        "        'file': os.path.basename(fpath),\n",
        "        **desc\n",
        "    })\n",
        "\n",
        "summary = pd.DataFrame(summary)\n",
        "summary"
      ],
      "id": "tbl-part-a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Period Estimation\n",
        "> * Apply autocorrelation or peak-detection algorithms\n",
        "> * Report mean period and standard deviation across trials: $\\overline{T_n} = T_n \\pm \\sigma_T$\n",
        "> * Compare with theoretical prediction (@eq-simple-pendulum)\n",
        "\n",
        "Throught these below code, we candetect the peak and We can obtain the time difference between two points before and after. We already get the each period, and then just need compute the mean period and standard deviation as shown in @tbl-mean-period-std-a."
      ],
      "id": "b23ca297"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-overview-a\n",
        "#| fig-cap: Data overview of part A\n",
        "#| echo: false\n",
        "L_short = 0.3  # meters (adjust if measured)\n",
        "# print(f\"Found {len(files)} files for Part A analysis.\")\n",
        "results = []\n",
        "\n",
        "# Create a 2x5 subplot grid for all trials\n",
        "fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
        "axes = axes.ravel()  # flatten axes array for easier indexing\n",
        "\n",
        "for idx, fpath in enumerate(files):\n",
        "    if idx >= 10:  # limit to 10 figures\n",
        "        break\n",
        "\n",
        "    df_A = load_trial_csv(fpath)\n",
        "    t = df_A['time'].values\n",
        "    x = df_A['accel'].values\n",
        "    \n",
        "    # Plot in the corresponding subplot\n",
        "    ax = axes[idx]\n",
        "    ax.plot(t, x, 'b-', alpha=0.7)\n",
        "    ax.set_title(f'Trial {idx+1}: {os.path.basename(fpath)}')\n",
        "    ax.set_xlabel('Time (s)')\n",
        "    ax.set_ylabel('Accel (units)')\n",
        "    ax.grid(True)\n",
        "plt.show()"
      ],
      "id": "fig-overview-a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute the mean period and standard deviation as shown in @tbl-mean-period-std-a."
      ],
      "id": "eeb1ea5e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tbl-mean-period-std-a\n",
        "#| tbl-cap: Part A analysis loop (short pendulum)\n",
        "#| echo: false\n",
        "for idx, fpath in enumerate(files):\n",
        "    # Continue with analysis (not shown in plot)\n",
        "    x_smooth = signal.savgol_filter(x, 11, 3)\n",
        "    periods, peaks = estimate_period_via_peaks(t, x_smooth, prominence=np.std(x_smooth)*0.5)\n",
        "    if isinstance(periods, np.ndarray):\n",
        "        mean_period = np.mean(periods)\n",
        "        std_period = np.std(periods, ddof=1)\n",
        "    else:\n",
        "        mean_period = np.nan\n",
        "        std_period = np.nan\n",
        "    # peak amplitudes and times\n",
        "    peak_times = t[peaks] if len(peaks)>0 else np.array([])\n",
        "    peak_amps = np.abs(x_smooth[peaks]) if len(peaks)>0 else np.array([])\n",
        "    decay = fit_exponential_decay(peak_times, peak_amps, Tn=mean_period)\n",
        "    result = {\n",
        "        'file': os.path.basename(fpath),\n",
        "        'mean_period': mean_period,\n",
        "        'std_period': std_period,\n",
        "        'A0': decay['A0'],\n",
        "        'k': decay['k'],\n",
        "        'zeta': decay['zeta'],\n",
        "        **desc\n",
        "    }\n",
        "    results.append(result)\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df"
      ],
      "id": "tbl-mean-period-std-a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The theoretical can easily compute as following via @eq-simple-pendulum:\n",
        "$$\n",
        "\\begin{align}\n",
        "T_{n}&=2\\pi\\sqrt{\\frac{L}{g}} \\\\\n",
        "&= 2\\pi\\sqrt{\\frac{0.32}{9.81}} \\\\\n",
        "&= 1.135 (s)\n",
        "\\end{align}\n",
        "$$"
      ],
      "id": "0698f7f7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| label: compare-part-a\n",
        "\n",
        "theoretical_period_a = 1.1348\n",
        "mean_period_a = float(np.round(results_df['mean_period'].mean(),4))\n",
        "std_period_a = float(np.round(results_df['mean_period'].std(ddof=1),4))\n",
        "diff_period_a = float(np.round(theoretical_period_a-mean_period_a,4))\n",
        "# print(f\"Mean period: {mean_period:.4f} s, Std: {std_period:.4f} s\")"
      ],
      "id": "compare-part-a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The 10 trials mean period is `{python} mean_period_a` s, and the standard deviation of mean period is `{python} std_period_a` s.\n",
        "\n",
        "Compare with theoretical, the diffirence is `{python} diff_period_a`. \n",
        "\n",
        "\n",
        "## Damping Analysis\n",
        "Using the last trial as example, We can use images to further reveal the characteristics of the data. @fig-simple-pendulum-analysis-1"
      ],
      "id": "0f8d4ca9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-simple-pendulum-analysis\n",
        "#| fig-cap: Create subplots for comparison\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Plot 1: Raw vs Smoothed Signal\n",
        "ax1.plot(t, x, 'b-', alpha=0.5, label='Raw')\n",
        "ax1.plot(t, x_smooth, 'r-', alpha=0.5, label='Smoothed')\n",
        "ax1.set_title('Signal Comparison')\n",
        "ax1.set_xlabel('Time (s)')\n",
        "ax1.set_ylabel('Acceleration')\n",
        "ax1.grid(True)\n",
        "ax1.legend()\n",
        "\n",
        "# Plot 2: Peak Detection\n",
        "ax2.plot(t, x_smooth, 'b-')\n",
        "if len(peaks) > 0:\n",
        "    ax2.plot(t[peaks], x_smooth[peaks], 'ro', label='Peaks')\n",
        "ax2.set_title('Peak Detection')\n",
        "ax2.set_xlabel('Time (s)')\n",
        "ax2.set_ylabel('Acceleration')\n",
        "ax2.grid(True)\n",
        "ax2.legend()\n",
        "\n",
        "# Plot 3: Period Analysis\n",
        "if isinstance(periods, np.ndarray) and len(periods) > 0:\n",
        "    ax3.plot(periods, 'g.-')\n",
        "    ax3.axhline(y=np.mean(periods), color='r', linestyle='--', \n",
        "                label=f'Mean: {np.mean(periods):.4f}s')\n",
        "    ax3.set_title('Period Variation')\n",
        "    ax3.set_xlabel('Peak Index')\n",
        "    ax3.set_ylabel('Period (s)')\n",
        "    ax3.grid(True)\n",
        "    ax3.legend()\n",
        "\n",
        "# Plot 4: Peak Amplitude Decay\n",
        "if len(peak_times) > 0:\n",
        "    ax4.plot(peak_times, peak_amps, 'b.', label='Peak Amplitudes')\n",
        "    ax4.set_title('Amplitude Decay')\n",
        "    ax4.set_xlabel('Time (s)')\n",
        "    ax4.set_ylabel('Peak Amplitude')\n",
        "    ax4.grid(True)\n",
        "    ax4.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "fig-simple-pendulum-analysis",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Important Tip:** We cannon get the pricise amplitudes, because the values we recorded are accelerator number.\n",
        "TODO: mybe we can use the accelerator inversely calculate the amplitude, but this exce the scope of this assignment."
      ],
      "id": "969dddaf"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-peak-amplitudes-part-a\n",
        "#| fig-cap: Peak amplitudes over time for 10 trials (Part A)\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "for idx, fpath in enumerate(files[:10]):\n",
        "    df = load_trial_csv(fpath)\n",
        "    t = df['time'].values\n",
        "    x = df['accel'].values\n",
        "    x_smooth = signal.savgol_filter(x, 11, 3)\n",
        "    periods, peaks = estimate_period_via_peaks(t, x_smooth, prominence=np.std(x_smooth)*0.5)\n",
        "    if len(peaks) > 0:\n",
        "        peak_times = t[peaks]\n",
        "        peak_amps = np.abs(x_smooth[peaks])\n",
        "        ax.plot(peak_times, peak_amps/0.31656, marker='o', linestyle='-', label=f'Trial {idx+1}', alpha=0.7)\n",
        "\n",
        "ax.set_xlabel('Time (s)')\n",
        "ax.set_ylabel('Peak Amplitude')\n",
        "ax.set_title('Peak Amplitudes Over Time (10 Trials, Part A)')\n",
        "ax.grid(True)\n",
        "ax.set_ylim(bottom=0)\n",
        "ax.legend(loc='upper right', ncol=2, fontsize='small')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "fig-peak-amplitudes-part-a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As shown in the @fig-peak-amplitudes-part-a, the amplitudes have respectively difference, due to the \n",
        "\n",
        "TODO: analyze the difference\n",
        "\n",
        "# Fit exponential decay to all 10 trials and report mean/std of damping ratio ζ"
      ],
      "id": "1e3d1b80"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "zeta_list = []\n",
        "for idx, fpath in enumerate(files[:10]):\n",
        "    df = load_trial_csv(fpath)\n",
        "    t = df['time'].values\n",
        "    x = df['accel'].values\n",
        "    x_smooth = signal.savgol_filter(x, 11, 3)\n",
        "    periods, peaks = estimate_period_via_peaks(t, x_smooth, prominence=np.std(x_smooth)*0.5)\n",
        "    if isinstance(periods, np.ndarray) and len(peaks) > 1:\n",
        "        peak_times = t[peaks]\n",
        "        peak_amps = np.abs(x_smooth[peaks])\n",
        "        mean_period = np.mean(periods)\n",
        "        decay = fit_exponential_decay(peak_times, peak_amps, Tn=mean_period)\n",
        "        zeta_list.append(decay['zeta'])\n",
        "\n",
        "zeta_arr = np.array(zeta_list)\n",
        "mean_zeta = np.nanmean(zeta_arr)\n",
        "std_zeta = np.nanstd(zeta_arr, ddof=1)\n",
        "\n",
        "print(f\"Mean damping ratio ζ: {mean_zeta:.4f} ± {std_zeta:.4f}\")"
      ],
      "id": "1fd38761",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Discussion\n",
        "\n",
        "\n",
        "# Part B: Long Pendulum (L ≈ 1.0 m)\n",
        "\n",
        "Repeat analysis from Part A for `partB_trialXX.csv` files. Adjust `L_long` below if your measured length differs.\n",
        "\n",
        "## Statistical Analysis\n",
        "\n",
        "## Damping Analysis\n",
        "\n",
        "## Discussion\n",
        "\n",
        "## Comparative Study"
      ],
      "id": "c21d4990"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: Part B long Pendulum\n",
        "L_long = 1.0\n",
        "file_pattern = os.path.join(DATA_DIR, 'partB_trial*.csv')\n",
        "files = sorted(glob.glob(file_pattern))\n",
        "# Copy-paste the same loop for Part B (or refactor into a function). For brevity we reuse code:\n",
        "results_B = []\n",
        "for fpath in files:\n",
        "    df = load_trial_csv(fpath)\n",
        "    t = df['time'].values\n",
        "    x = df['accel'].values\n",
        "    x_smooth = signal.savgol_filter(x, 11, 3)\n",
        "    periods, peaks = estimate_period_via_peaks(t, x_smooth, prominence=np.std(x_smooth)*0.5)\n",
        "    if isinstance(periods, np.ndarray):\n",
        "        mean_period = np.mean(periods)\n",
        "        std_period = np.std(periods, ddof=1)\n",
        "    else:\n",
        "        mean_period = np.nan\n",
        "        std_period = np.nan\n",
        "    peak_times = t[peaks] if len(peaks)>0 else np.array([])\n",
        "    peak_amps = np.abs(x_smooth[peaks]) if len(peaks)>0 else np.array([])\n",
        "    decay = fit_exponential_decay(peak_times, peak_amps, Tn=mean_period)\n",
        "    desc = compute_descriptors(x)\n",
        "    result = {\n",
        "        'file': os.path.basename(fpath),\n",
        "        'mean_period': mean_period,\n",
        "        'std_period': std_period,\n",
        "        'A0': decay['A0'],\n",
        "        'k': decay['k'],\n",
        "        'zeta': decay['zeta'],\n",
        "        **desc\n",
        "    }\n",
        "    results_B.append(result)\n",
        "pd.DataFrame(results_B)"
      ],
      "id": "Part-B-long-Pendulum",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part C: Nonlinear Regime (Large Angles)\n",
        "\n",
        "Use `partC_trialXX.csv`. For large angle trials (θ0 > 45°), measure period variation vs amplitude and compare with small-angle theory.\n",
        "\n",
        "## Period Analysis\n",
        "\n",
        "## Comparison and Discussion\n"
      ],
      "id": "cd5300d5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "file_pattern = os.path.join(DATA_DIR, 'partC_trial*.csv')\n",
        "files = sorted(glob.glob(file_pattern))\n",
        "period_vs_amp = []\n",
        "for fpath in files:\n",
        "    df = load_trial_csv(fpath)\n",
        "    t = df['time'].values\n",
        "    x = df['accel'].values\n",
        "    x_smooth = signal.savgol_filter(x, 11, 3)\n",
        "    periods, peaks = estimate_period_via_peaks(t, x_smooth, prominence=np.std(x_smooth)*0.5)\n",
        "    peak_times = t[peaks] if len(peaks)>0 else np.array([])\n",
        "    peak_amps = np.abs(x_smooth[peaks]) if len(peaks)>0 else np.array([])\n",
        "    if len(peak_amps)>0 and isinstance(periods, np.ndarray):\n",
        "        period_vs_amp.append({'file': os.path.basename(fpath), 'mean_period': np.mean(periods), 'mean_amp': np.mean(peak_amps)})\n",
        "pd.DataFrame(period_vs_amp)"
      ],
      "id": "0cb9b6e2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part D: Noise Discussion\n",
        "\n",
        "Visually inspect signals and write your qualitative assessment here. Use the examples shown in previous plots. Answer: What are possible noise sources? How do they affect period/damping estimation?\n",
        "\n",
        "## Qualitative Noise Observation\n",
        "\n",
        "### Discussion\n",
        "\n",
        "\n",
        "\n",
        "# Submission Checklist\n",
        "\n",
        "- [ ] Export this notebook to PDF: `HW1_{STUDENT_ID}.pdf` (max 20 A4 pages)\n",
        "- [ ] Create ZIP named `HW1_{STUDENT_ID}.zip` containing:\n",
        "  - Executed `.ipynb`\n",
        "  - `Data/` folder with CSVs\n",
        "  - `Figures/` folder with generated plots\n",
        "- [ ] Verify all tables are rendered as DataFrames (no screenshots)\n",
        "- [ ] Confirm all equations and figures are numbered and captioned\n",
        "- [ ] Ensure reproducibility: all code cells run from top to bottom without manual path edits (except `STUDENT_ID`)\n"
      ],
      "id": "84a4469d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Helper: save summary tables and figures\n",
        "summary_csv = os.path.join(BASE_DIR, f'summary_{STUDENT_ID}.csv')\n",
        "if 'results_df' in globals():\n",
        "    results_df.to_csv(os.path.join(BASE_DIR, 'summary_partA.csv'), index=False)\n",
        "if 'results_B' in globals():\n",
        "    pd.DataFrame(results_B).to_csv(os.path.join(BASE_DIR, 'summary_partB.csv'), index=False)\n",
        "print('Saved summary CSVs to', BASE_DIR)"
      ],
      "id": "62dc4bd5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Reproduce Guide\n",
        "## How to Reproduce This Analysis\n",
        "\n",
        "1. **Install Required Software**\n",
        "    - Install [Python 3.8+](https://www.python.org/downloads/).\n",
        "    - Install [Quarto](https://quarto.org/docs/get-started/).\n",
        "    - (Optional) Install [JupyterLab](https://jupyter.org/) for interactive editing.\n",
        "\n",
        "2. **Set Up the Environment**\n",
        "    - Open a terminal in the project root folder.\n",
        "    - (Recommended) Create a virtual environment:\n",
        "      ```\n",
        "      python -m venv .venv\n",
        "      source .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n",
        "      ```\n",
        "    - Install dependencies:\n",
        "      ```\n",
        "      pip install -r requirements.txt\n",
        "      ```\n",
        "\n",
        "3. **Prepare Data**\n",
        "    - Place your raw data CSV files in the `Data/` folder as described in the instructions.\n",
        "    - Ensure filenames match the expected patterns (e.g., `partA_trial01.csv`).\n",
        "\n",
        "4. **Edit Metadata**\n",
        "    - Open `HW1_STUDENTID.qmd` and set your `STUDENT_ID`, name, and date at the top.\n",
        "\n",
        "5. **Render the Report**\n",
        "    - To render as HTML:\n",
        "      ```\n",
        "      quarto render HW1_STUDENTID.qmd --to html\n",
        "      ```\n",
        "    - To render as PDF (requires a LaTeX installation):\n",
        "      ```\n",
        "      quarto render HW1_STUDENTID.qmd --to pdf\n",
        "      ```\n",
        "\n",
        "6. **Troubleshooting**\n",
        "    - If you encounter missing package errors, install them with `pip install <package>`.\n",
        "    - Ensure all code cells run from top to bottom without manual edits except for `STUDENT_ID`.\n",
        "\n",
        "7. **Export and Submit**\n",
        "    - Export the rendered PDF and required files as described in the submission checklist.\n",
        "\n",
        "**Tip:** For interactive exploration, you can open the `.qmd` file in JupyterLab or VS Code with the Quarto extension.\n",
        "\n",
        "**Contact:** If you have issues reproducing the results, check your Python/Quarto versions and data file structure."
      ],
      "id": "cb5bea92"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "C:\\Users\\jinpeng6\\.conda\\envs\\hw1\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}